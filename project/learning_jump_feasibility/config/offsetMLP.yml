DATA: # arguments that will be passed to get_dataloaders
  name: "offset"
  batch_size: 1024
  noise_std: 1.0e-4
  exclude: "pos_vj"
  data_path: "/home/akizhanov/esfal/data/learning_jump_feasibility"

MODEL:
  model_name: MLP
  PARAMS: # arguments that will be passed to the chosen models
    input_dim: 46
    output_dim: 12
    n_hidden: 3
    latent_dim: 128
    activation: ReLU

TRAINING: # arguments that will be passed to the trainer
  criterion_str: L1Loss # MSELoss # BCEWithLogitsLoss # 
  optimizer:
    optimizer_name: Adam
    PARAMS:
      lr: 0.001
  epochs: 75
  logdir: "./logs/MLP_offset/"
  use_logger: True
  ckpt_every: -1
  lr_scheduler:
    lr_scheduler_name: ExponentialLR
    PARAMS:
      gamma: 0.98
  # device: "cpu"

# SWEEP: # arguments to sweep over and their values
#   latent_dim: [16, 32, 64]
#   n_hidden: [3, 4]
#   batch_size: [32, 64, 128]
  # lr:
  #   min: 0.0001
  #   max: 0.001
  #   n: 2
  #   logspace: True